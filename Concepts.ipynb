{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0873d37-00ba-4626-9743-a23511736b0f",
   "metadata": {},
   "source": [
    "# Neural networks consist of many Linear Transformations\n",
    "\n",
    "A linear transformation takes the form\n",
    "\n",
    "$ Y = X \\cdot W + b $\n",
    "\n",
    "**W** and **b** are the weight and bias matrices and **X** is the input matrix.  **Y** is the output of the **linear transformation**.\n",
    "\n",
    "A linear transformation is a way to change a set of points (vectors) in space so that lines remain lines, and the origin remains fixed. In simpler terms, it's a rule for moving every point to a specific new location without twisting, warping, or tearing the shape formed by those points. In machine learning, this is often done using matrices to systematically shift, rotate, or scale the input data.\n",
    "\n",
    "In the code below, notice W is passed to matmul with the `.t()` function.  This transposes the matrix W.  In the first instance of the Adder model, X (the input) has a **shape** of (1,2) and W has a shape of (4, 2).  Matrix multiplication requires that **broadcasting rules** be followed.  Taking the transposed form of W changes its shape to (2, 4).  The result (Y) of the matrix multiplication is a matrix of (1, 4).  (1,2) * (2,4) = (1,4)\n",
    "\n",
    "```\n",
    "Y = torch.matmul(X, W.t()) + b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2e6e7d-7e67-4d0f-85bd-a5314679553c",
   "metadata": {},
   "source": [
    "# Nonlinearity is introduced through Activation Functions\n",
    "\n",
    "Activation functions introduce nonlinearity into neural networks, allowing them to learn from the error and make adjustments, which enables the model to handle more complex data patterns.\n",
    "\n",
    "Here's a list of commonly used activation functions, their brief descriptions, ideal usage scenarios, and how to use them in PyTorch:\n",
    "\n",
    "1. **ReLU (Rectified Linear Unit)**\n",
    "    - **Description**: ReLU replaces all negative values in the vector with zero.\n",
    "    - **Where to Use**: Hidden layers in most networks.\n",
    "    - **PyTorch Code**: `torch.nn.ReLU()`\n",
    "\n",
    "2. **Sigmoid**\n",
    "    - **Description**: Maps input values to the range of 0 to 1.\n",
    "    - **Where to Use**: Binary classification output layer.\n",
    "    - **PyTorch Code**: `torch.nn.Sigmoid()`\n",
    "\n",
    "3. **Tanh (Hyperbolic Tangent)**\n",
    "    - **Description**: Maps input to a range between -1 and 1.\n",
    "    - **Where to Use**: Hidden layers when outputs may be negative.\n",
    "    - **PyTorch Code**: `torch.nn.Tanh()`\n",
    "\n",
    "4. **Softmax**\n",
    "    - **Description**: Converts a real vector to a probability distribution.\n",
    "    - **Where to Use**: Multi-class classification output layer.\n",
    "    - **PyTorch Code**: `torch.nn.Softmax(dim=1)`\n",
    "\n",
    "5. **Leaky ReLU**\n",
    "    - **Description**: Similar to ReLU but allows a small gradient for negative values.\n",
    "    - **Where to Use**: Hidden layers to avoid dead neurons.\n",
    "    - **PyTorch Code**: `torch.nn.LeakyReLU()`\n",
    "\n",
    "6. **Swish**\n",
    "    - **Description**: Self-gated activation function.\n",
    "    - **Where to Use**: Hidden layers, generally outperforms ReLU on deeper networks.\n",
    "    - **PyTorch Code**: Custom function or `torch.nn.SiLU()` (Swish implementation in PyTorch)\n",
    "\n",
    "7. **ELU (Exponential Linear Unit)**\n",
    "    - **Description**: Similar to ReLU but takes care of the vanishing gradient problem for negative inputs.\n",
    "    - **Where to Use**: Hidden layers when faster learning is needed.\n",
    "    - **PyTorch Code**: `torch.nn.ELU()`\n",
    "\n",
    "To use any of these in a PyTorch model, you can add them as a layer, e.g., `self.act1 = torch.nn.ReLU()`, and then apply them in your `forward` method: `x = self.act1(x)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53f6b11-521b-4867-a71e-c1f65a054555",
   "metadata": {},
   "source": [
    "# Broadcasting Rules\n",
    "\n",
    "If you understand broadcasting rules, it can help to make sense of when to transpose a matrix and what types of shape transformations can happen.  [Andrej Karpathy's Zero to Hero course](https://karpathy.ai/zero-to-hero.html) explains this well!\n",
    "\n",
    "[Read this to understand broadcasting rules](https://numpy.org/doc/stable/user/basics.broadcasting.html).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e154eb-0c74-4412-a664-68c8823a63e3",
   "metadata": {},
   "source": [
    "# Neural networks usually start out with random values that need trained\n",
    "\n",
    "A model is **trained** by having its weights adjusted by small amounts so that the **accuracy** improves and the **loss** shrinks.  During training, it is common for the **loss** to grow instead of shrink in some cases.  This happens when the model potentially skips over the ideal weight combination or finds itself in a local minima.  \n",
    "\n",
    "\n",
    "# Gradient Descent and Learning Rate\n",
    "\n",
    "The process of applying gradient descent to a neural network deposits gradients to each parameter of the network.  The gradient is a floating point number indicating the direction the parameter in the network needs to change to improve **accuracy** and reduce **loss**.  Once the gradients are distributed through the network, they are applied by multiplying the **negative** gradient by the weight or bias and a **learning rate**.  The negative is why the process is called **descent**.  When numbers are multiplied and one of them is negative, it makes the entire result negative.  The gradient is like a slope and if the negative wasn't used, the result would continue to grow away from the desired result.\n",
    "\n",
    "# Optimizer\n",
    "\n",
    "The gradients are deposited to each weight and depending upon the network setup a multiple gradients may be deposited to the same weight.  When this happens, the gradients are simply summed together.  For this reason, before depositing gradients it is important to set them all to zero.  In PyTorch this is done by using an **optimizer**. \n",
    "\n",
    "# Learning Rate\n",
    "\n",
    "The **learning rate** is typically a small number 0.01E-4 to 0.01E-5.  The primary reason for the learning rate being so small is to prevent the model from skipping past the best settings.  There is a lot that can be done with learning rates including making them dynamic, altering them based upon the given epoch.\n",
    "\n",
    "# Epoch\n",
    "\n",
    "An epoch is one complete pass through the entire training dataset. During an epoch, the model's parameters are updated iteratively using subsets of the training data, often referred to as batches. Multiple epochs are usually necessary to sufficiently train a model.  In the code below, 2000 epochs are used training on the single equation `30+70 = 100`.\n",
    "\n",
    "# Hyperparameters\n",
    "\n",
    "A hyperparameter is a parameter whose value is set before the training process begins, as opposed to the parameters of the model, which are learned during training. Examples include learning rate, batch size, and number of epochs. Hyperparameters are often tuned to optimize model performance.\n",
    "\n",
    "# Loss function\n",
    "\n",
    "The loss function (often called criterion) is used to compare the model's prediction to the actual value.  In the example below, the loss function is the MSELoss() function.  The MSELoss function calculates the average of the squares of the differences between predicted and actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c80027-bea5-4d6f-8629-4d58cb026dfb",
   "metadata": {},
   "source": [
    "# Overfitting\n",
    "\n",
    "Overfitting occurs when a model learns the training data too well, capturing noise and anomalies, rather than the underlying pattern. As a result, it performs poorly on new, unseen data. Overfitting is often a sign that a model is too complex relative to the simplicity of the problem. Techniques like **regularization**, **dropout**, and **simpler architectures** can help mitigate overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbfb084-15ee-43b1-9cea-3d08b2f3d669",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "The process of applying gradient descent to a neural network deposits gradients to each parameter of the network.  The gradient is a floating point number indicating the direction the parameter in the network needs to change to improve **accuracy** and reduce **loss**.  Once the gradients are distributed through the network, they are applied by multiplying the **negative** gradient by the weight or bias and a **learning rate**.  The negative is why the process is called **descent**.  When numbers are multiplied and one of them is negative, it makes the entire result negative.  The gradient is like a slope and if the negative wasn't used, the result would continue to grow away from the desired result.\n",
    "\n",
    "Models are trained using **gradient descent**.  The \"gradient\" in the name refers to the derivative of the function at the current point. The algorithm takes steps proportional to the negative of the gradient, moving towards the minimum of the function.\n",
    "\n",
    "The derivative of a function at a given point is essentially the slope of the function at that point. In the context of a function $ ( f(x) ) $ of a single variable, the derivative $ ( f'(x) ) $represents the slope of the tangent line to the function at a specific point $ ( x ) $. This slope indicates how the function is changing at that point. For functions of more than one variable, the concept of a derivative generalizes to partial derivatives.\n",
    "\n",
    "A partial derivative is the derivative of a function of multiple variables with respect to one of those variables, keeping the other variables constant. For a function $ ( f(x, y, z, \\ldots) ) $, the partial derivative with respect to $ ( x ) $ would indicate how $ ( f ) $ changes as $ ( x ) $ changes, while keeping $ ( y, z, \\ldots ) $ constant. It is denoted as $ ( \\frac{\\partial f}{\\partial x} ) $ for the variable $ ( x ) $.\n",
    "\n",
    "Partial derivatives are used to form the gradient vector, which combines all the partial derivatives of a function into a single vector. This is useful in multivariable calculus and optimization problems, including machine learning algorithms like gradient descent.\n",
    "\n",
    "The chain rule extends to functions of multiple variables through the use of partial derivatives. This generalizes to more complicated functions and is a cornerstone of backpropagation in neural networks, where it's used to compute gradients of a loss function with respect to the weights.\n",
    "\n",
    "[Andrej Karpathy's Zero to Hero course](https://karpathy.ai/zero-to-hero.html) explains this well!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a20bc1-4fea-4b29-8db4-a7fb64db9274",
   "metadata": {},
   "source": [
    "# Loss Functions\n",
    "\n",
    "Here's the list of common loss functions with their corresponding PyTorch functions:\n",
    "\n",
    "Each loss function has its own characteristics and is suited for specific types of problems. Choosing the right loss function is crucial for training an effective model.\n",
    "\n",
    "1. **Mean Squared Error (MSE)**\n",
    "   - **PyTorch Function**: `torch.nn.MSELoss()`\n",
    "   - **Description**: Calculates the average of the squares of the differences between predicted and actual values.\n",
    "   - **Best Suited For**: Regression problems.\n",
    "\n",
    "2. **Mean Absolute Error (MAE)**\n",
    "   - **PyTorch Function**: `torch.nn.L1Loss()`\n",
    "   - **Description**: Calculates the average of the absolute differences between predicted and actual values.\n",
    "   - **Best Suited For**: Regression problems with outliers.\n",
    "\n",
    "3. **Cross-Entropy Loss (Log Loss)**\n",
    "   - **PyTorch Function**: `torch.nn.CrossEntropyLoss()`\n",
    "   - **Description**: Measures the performance of a classification model, rewarding confidence in correct classifications.\n",
    "   - **Best Suited For**: Binary and multi-class classification.\n",
    "\n",
    "4. **Hinge Loss**\n",
    "   - **PyTorch Function**: `torch.nn.HingeEmbeddingLoss()`\n",
    "   - **Description**: Used for \"maximum-margin\" classification, particularly for support vector machines.\n",
    "   - **Best Suited For**: Binary classification.\n",
    "\n",
    "5. **Categorical Cross-Entropy**\n",
    "   - **PyTorch Function**: `torch.nn.CrossEntropyLoss()`\n",
    "   - **Description**: Extension of Cross-Entropy loss for multi-class classification problems.\n",
    "   - **Best Suited For**: Multi-class classification with single label.\n",
    "\n",
    "6. **Kullback-Leibler Divergence**\n",
    "   - **PyTorch Function**: `torch.nn.KLDivLoss()`\n",
    "   - **Description**: Measures how one probability distribution diverges from another.\n",
    "   - **Best Suited For**: Multi-class classification, recommendation systems.\n",
    "\n",
    "7. **Poisson Loss**\n",
    "   - **PyTorch Function**: `torch.nn.PoissonNLLLoss()`\n",
    "   - **Description**: Measures the difference between the predicted average occurrence rate and the actual rate.\n",
    "   - **Best Suited For**: Count-based regression problems.\n",
    "\n",
    "8. **Cosine Similarity**\n",
    "   - **PyTorch Function**: Use `torch.nn.CosineSimilarity()` and create a custom loss\n",
    "   - **Description**: Measures the cosine of the angle between the predicted and actual vectors to measure similarity.\n",
    "   - **Best Suited For**: Text similarity, clustering.\n",
    "\n",
    "9. **Huber Loss**\n",
    "   - **PyTorch Function**: `torch.nn.SmoothL1Loss()`\n",
    "   - **Description**: Combination of MAE and MSE; less sensitive to outliers than MSE.\n",
    "   - **Best Suited For**: Regression problems with occasional outliers.\n",
    "\n",
    "10. **Negative Log-Likelihood (NLL)**\n",
    "    - **PyTorch Function**: `torch.nn.NLLLoss()`\n",
    "    - **Description**: Similar to Cross-Entropy but without logit transformation; often used with Softmax.\n",
    "    - **Best Suited For**: Classification problems with Softmax.\n",
    "\n",
    "11. **Focal Loss**\n",
    "    - **PyTorch Function**: Not built-in; custom implementation required.\n",
    "    - **Description**: Modification of Cross-Entropy that gives more weight to hard-to-classify examples.\n",
    "    - **Best Suited For**: Imbalanced classification problems.\n",
    "\n",
    "12. **Dice Loss**\n",
    "    - **PyTorch Function**: Not built-in; custom implementation required.\n",
    "    - **Description**: Measures overlap between predicted and ground truth sets; often used in image segmentation.\n",
    "    - **Best Suited For**: Image segmentation tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f2dfff-21b7-4fd9-aca1-be9e0eea8248",
   "metadata": {},
   "source": [
    "# Mean Squared Error (MSE) Loss Function\n",
    "\n",
    "- **Essence**: Measures the average of the squares of the errors or deviations between predicted and actual values.\n",
    "\n",
    "- **Why Use**: Suitable for regression problems or tasks where predicting the exact numeric value is important. It penalizes larger errors more significantly than smaller ones.\n",
    "\n",
    "- **PyTorch Code**:\n",
    "  ```python\n",
    "  criterion = torch.nn.MSELoss()\n",
    "  ```\n",
    "\n",
    "- **How It Works**: The formula for MSE is:\n",
    "\n",
    "  $ [\n",
    "  \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "  ] $\n",
    "  \n",
    "  Here, $y_i$ is the actual value and $\\hat{y}_i$ is the predicted value. $n$ is the total number of samples.\n",
    "\n",
    "The MSE loss function is sensitive to outliers and might not be suitable for data with heavy-tailed distributions. However, it is one of the most commonly used loss functions for regression tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835d9fdd-b552-4682-8e3a-70aee7a040b5",
   "metadata": {},
   "source": [
    "# Mean Absolute Error (MAE) Loss Function\n",
    "\n",
    "- **Essence**: Measures the average of the absolute differences between predicted and actual values.\n",
    "\n",
    "- **Why Use**: Suitable for regression problems, particularly when you want to be less sensitive to outliers compared to MSE.\n",
    "\n",
    "- **PyTorch Code**:\n",
    "  ```python\n",
    "  criterion = torch.nn.L1Loss()\n",
    "  ```\n",
    "\n",
    "- **How It Works**: The formula for MAE is:\n",
    "\n",
    "  $[\n",
    "  \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
    "  ]$\n",
    "\n",
    "  Here, $y_i$ is the actual value and $\\hat{y}_i$ is the predicted value. $n$ is the total number of samples.\n",
    "\n",
    "MAE is more robust to outliers compared to MSE and is often used when the distribution of errors or residuals is not normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfd4d95-b6af-4fc7-96ed-b18e345d89a3",
   "metadata": {},
   "source": [
    "# Cross-Entropy Loss (Log Loss) Loss Function\n",
    "\n",
    "- **Essence**: Measures the dissimilarity between the true label distribution and the predicted probabilities in classification tasks.\n",
    "\n",
    "- **Why Use**: Commonly used in binary and multi-class classification problems where outputs can be interpreted as probabilities.\n",
    "\n",
    "- **PyTorch Code**:\n",
    "  ```python\n",
    "  criterion = torch.nn.CrossEntropyLoss()  # For multi-class\n",
    "  criterion = torch.nn.BCEWithLogitsLoss()  # For binary classification\n",
    "  ```\n",
    "\n",
    "- **How It Works**: For binary classification, the formula is:\n",
    "\n",
    "  $[\n",
    "  \\text{Cross-Entropy Loss} = -\\frac{1}{n} \\sum_{i=1}^{n} [y_i \\log(\\hat{y}_i) + (1-y_i) \\log(1-\\hat{y}_i)]\n",
    "  ]$\n",
    "\n",
    "  For multi-class classification:\n",
    "\n",
    "  $[\n",
    "  \\text{Cross-Entropy Loss} = -\\frac{1}{n} \\sum_{i=1}^{n} \\sum_{c=1}^{C} y_{ic} \\log(\\hat{y}_{ic})\n",
    "  ]$\n",
    "\n",
    "  Here, $y_{ic}$ is 1 if the $i$-th sample belongs to class $c$ and 0 otherwise. $\\hat{y}_{ic}$ is the predicted probability that the $i$-th sample belongs to class $c$.\n",
    "\n",
    "Cross-Entropy Loss is particularly useful when the output can be interpreted as the probability of belonging to certain classes. It heavily penalizes predictions that are confidently wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17449993-34c6-4cc2-bfa8-0d41b1808225",
   "metadata": {},
   "source": [
    "# Optimizers\n",
    "\n",
    "Below are some common optimizers, their characteristics, and example PyTorch code snippets to initialize them:\n",
    "\n",
    "### SGD (Stochastic Gradient Descent)\n",
    "- **Why**: Simplicity and ease of implementation.\n",
    "- **Essence**: Updates parameters using the gradient of the loss function.\n",
    "- **Code**: \n",
    "  ```python\n",
    "  optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "  ```\n",
    "\n",
    "### Momentum\n",
    "- **Why**: Faster convergence compared to plain SGD.\n",
    "- **Essence**: Adds a momentum term to SGD, which considers past gradients.\n",
    "- **Code**:\n",
    "  ```python\n",
    "  optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "  ```\n",
    "\n",
    "### Adagrad\n",
    "- **Why**: Suitable for sparse data, adjusts learning rates.\n",
    "- **Essence**: Scales learning rate for each parameter individually.\n",
    "- **Code**:\n",
    "  ```python\n",
    "  optimizer = torch.optim.Adagrad(model.parameters(), lr=0.01)\n",
    "  ```\n",
    "\n",
    "### RMSprop\n",
    "- **Why**: Good for non-stationary objectives.\n",
    "- **Essence**: Adapts learning rates during training.\n",
    "- **Code**:\n",
    "  ```python\n",
    "  optimizer = torch.optim.RMSprop(model.parameters(), lr=0.01)\n",
    "  ```\n",
    "\n",
    "### Adam\n",
    "- **Why**: Good default for many problems.\n",
    "- **Essence**: Combines features of Momentum and RMSprop.\n",
    "- **Code**:\n",
    "  ```python\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "  ```\n",
    "\n",
    "### AdamW\n",
    "- **Why**: Improves generalization.\n",
    "- **Essence**: Similar to Adam but with weight decay.\n",
    "- **Code**:\n",
    "  ```python\n",
    "  optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "  ```\n",
    "\n",
    "Each of these optimizers can be more effective depending on the specific problem, architecture, or data you are working with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b01fc21-3d69-4f48-9ba3-0112bcaf96d1",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent (SGD) Optimizer\n",
    "\n",
    "- **Essence**: Unlike traditional Gradient Descent that computes the gradient using the entire dataset, SGD estimates the gradient using a single or a small batch of samples. This makes each update faster but noisier.\n",
    "\n",
    "- **Why Use**: Faster computation and ability to escape local minima due to the noisy updates. Useful when you have a very large dataset.\n",
    "\n",
    "- **PyTorch Code**: \n",
    "  ```python\n",
    "  optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "  ```\n",
    "\n",
    "- **How It Works**: The weight update formula is the same as in traditional gradient descent:\n",
    "  ``` \n",
    "  w = w - learning_rate * estimated_gradient\n",
    "  ```\n",
    "  The key difference is that `estimated_gradient` is calculated using a subset of the entire dataset.\n",
    "\n",
    "The main advantage of SGD is computational efficiency, allowing for faster iterations and suitability for large-scale data. The randomness in choosing mini-batches can also help escape local minima for non-convex problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f041c0-2fe5-4e93-a98f-3de862524792",
   "metadata": {},
   "source": [
    "# Momentum Optimizer\n",
    "\n",
    "- **Essence**: Momentum helps the optimizer to navigate along the relevant directions and softens the oscillations in the irrelevant directions. It accumulates past gradients and uses them to make future updates, thereby adding inertia to the optimization process.\n",
    "  \n",
    "- **Why Use**: It speeds up the convergence and mitigates problems like getting stuck in local minima or oscillating.\n",
    "\n",
    "- **PyTorch Code**: \n",
    "  ```python\n",
    "  optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "  ```\n",
    "  \n",
    "- **How It Works**: Traditional gradient descent updates weights (`w`) as follows:\n",
    "  ``` \n",
    "  w = w - learning_rate * gradient\n",
    "  ```\n",
    "  Momentum modifies this by introducing a velocity (`v`) term:\n",
    "  ```\n",
    "  v = momentum * v - learning_rate * gradient\n",
    "  w = w + v\n",
    "  ```\n",
    "\n",
    "The `momentum` term usually has a value between 0 and 1; a typical value is 0.9. The velocity `v` is initialized as zero, and subsequently updated with the weighted sum of the negative gradient and the previous velocity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afafb42-81e9-4238-b87a-125d6dbc639a",
   "metadata": {},
   "source": [
    "# Adagrad (Adaptive Gradient Algorithm) Optimizer\n",
    "\n",
    "- **Essence**: Adagrad adapts the learning rates for each parameter individually based on the historical gradient information.\n",
    "\n",
    "- **Why Use**: Suitable for problems with features that have different frequencies. Good for sparse data and NLP tasks.\n",
    "\n",
    "- **PyTorch Code**: \n",
    "  ```python\n",
    "  optimizer = torch.optim.Adagrad(model.parameters(), lr=0.01)\n",
    "  ```\n",
    "\n",
    "- **How It Works**: The weight update formula is:\n",
    "  ```\n",
    "  G_t = G_{t-1} + (gradient_t)^2\n",
    "  w = w - (learning_rate / sqrt(G_t + epsilon)) * gradient_t\n",
    "  ```\n",
    "  where \\( G_t \\) is the sum of the squares of the past gradients and \\( \\epsilon \\) is a smoothing term to prevent division by zero.\n",
    "\n",
    "Adagrad adjusts the effective learning rate for each parameter, which can be beneficial for problems where some features are sparse and need more aggressive updates. However, the learning rate may decrease too fast, effectively stopping learning; thus, it's not always suitable for all problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294536a5-c573-49d5-b392-1adcf6443fbf",
   "metadata": {},
   "source": [
    "# RMSprop (Root Mean Square Propagation) Optimizer\n",
    "\n",
    "- **Essence**: RMSprop adjusts the learning rate during training, with a bias toward more recent gradients. It aims to resolve Adagrad's rapidly decreasing learning rate.\n",
    "\n",
    "- **Why Use**: Useful for non-stationary objectives and online learning scenarios. Also effective for problems that are sensitive to parameter updates, such as recurrent neural networks (RNNs).\n",
    "\n",
    "- **PyTorch Code**: \n",
    "  ```python\n",
    "  optimizer = torch.optim.RMSprop(model.parameters(), lr=0.01)\n",
    "  ```\n",
    "\n",
    "- **How It Works**: The weight update formula is:\n",
    "  ```\n",
    "  E[g^2]_t = (1 - decay_rate) * (gradient_t)^2 + decay_rate * E[g^2]_{t-1}\n",
    "  w = w - (learning_rate / sqrt(E[g^2]_t + epsilon)) * gradient_t\n",
    "  ```\n",
    "  where \\( E[g^2]_t \\) is the moving average of the square of the gradient, and \\( \\epsilon \\) is a smoothing term to prevent division by zero.\n",
    "\n",
    "RMSprop combines the benefits of both AdaGrad and AdaDelta. It uses a moving average of squared gradients to normalize the gradient itself. That means the step size is decided on a per-parameter basis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0f180b-4abd-4052-8061-31897310e64c",
   "metadata": {},
   "source": [
    "# Adam (Adaptive Moment Estimation) Optimizer\n",
    "\n",
    "- **Essence**: Combines the advantages of both Momentum and RMSprop. It calculates adaptive learning rates for each parameter and also keeps an exponentially decaying average of past gradients, similar to momentum.\n",
    "\n",
    "- **Why Use**: Effective in practice and suitable for most non-convex optimization problems. Good for deep learning and complex architectures.\n",
    "\n",
    "- **PyTorch Code**:\n",
    "  ```python\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "  ```\n",
    "\n",
    "- **How It Works**: The weight update formula combines terms from both Momentum and RMSprop:\n",
    "  ```\n",
    "  m_t = beta1 * m_{t-1} + (1 - beta1) * gradient_t\n",
    "  v_t = beta2 * v_{t-1} + (1 - beta2) * (gradient_t)^2\n",
    "  \n",
    "  m_t_hat = m_t / (1 - beta1^t)\n",
    "  v_t_hat = v_t / (1 - beta2^t)\n",
    "\n",
    "  w = w - learning_rate * m_t_hat / (sqrt(v_t_hat) + epsilon)\n",
    "  ```\n",
    "  where \\(m_t\\) and \\(v_t\\) are estimates of the first moment (the mean) and the second moment (the uncentered variance) of the gradients respectively.\n",
    "\n",
    "Adam works well in practice, outperforming most other optimizers in machine learning and deep learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926c0147-c3f6-4b21-89e5-7af436aa0add",
   "metadata": {},
   "source": [
    "# AdamW (Weight Decay Adam) Optimizer\n",
    "\n",
    "- **Essence**: A modification of the original Adam optimizer that decouples weight decay from the optimization steps. This corrects the weight decay calculation in the original Adam algorithm.\n",
    "\n",
    "- **Why Use**: Effective when you require L2 regularization in your models, especially in deep learning tasks. AdamW is often better than Adam for tasks that require a sparse representation.\n",
    "\n",
    "- **PyTorch Code**:\n",
    "  ```python\n",
    "  optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "  ```\n",
    "\n",
    "- **How It Works**: AdamW separates the weight decay term from the parameter update:\n",
    "  ```\n",
    "  m_t = beta1 * m_{t-1} + (1 - beta1) * gradient_t\n",
    "  v_t = beta2 * v_{t-1} + (1 - beta2) * (gradient_t)^2\n",
    "\n",
    "  m_t_hat = m_t / (1 - beta1^t)\n",
    "  v_t_hat = v_t / (1 - beta2^t)\n",
    "\n",
    "  w = (w - weight_decay * lr * w) - lr * m_t_hat / (sqrt(v_t_hat) + epsilon)\n",
    "  ```\n",
    "  This makes the optimizer more suitable for fine-tuning and helps in achieving better generalization.\n",
    "\n",
    "AdamW is particularly useful in scenarios where weight decay regularization is required, as it corrects the shortcomings of how weight decay is handled in the original Adam optimizer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
