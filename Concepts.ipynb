{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0873d37-00ba-4626-9743-a23511736b0f",
   "metadata": {},
   "source": [
    "# Neural networks consist of many Linear Transformations\n",
    "\n",
    "Notice that in each layer the computation is basically the same taking the form\n",
    "\n",
    "$ Y = X \\cdot W + b $\n",
    "\n",
    "**W** and **b** are the weight and bias matrices and **X** is the input matrix.  **Y** is the output of the **linear transformation**.\n",
    "\n",
    "A linear transformation is a way to change a set of points (vectors) in space so that lines remain lines, and the origin remains fixed. In simpler terms, it's a rule for moving every point to a specific new location without twisting, warping, or tearing the shape formed by those points. In machine learning, this is often done using matrices to systematically shift, rotate, or scale the input data.\n",
    "\n",
    "In the above code, the function used the `t()` function which transposes a matrix.  In the first instance, the input_tensor has a **shape** of (1,2) and the W_input has a shape of (4, 2).  Matrix multiplication requires that **broadcasting rules** be followed.  Taking the transposed form of W_input changes its shape to (2, 4).  The result (Y_input) of the matrix multiplication is a matrix of (1, 4).  \n",
    "\n",
    "```\n",
    "Y_input = torch.matmul(input_tensor, W_input.t()) + b_input\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53f6b11-521b-4867-a71e-c1f65a054555",
   "metadata": {},
   "source": [
    "# Broadcasting Rules\n",
    "\n",
    "If you understand broadcasting rules, it can help to make sense of when to transpose a matrix and what types of shape transformations can happen.  [Andrej Karpathy's Zero to Hero course](https://karpathy.ai/zero-to-hero.html) explains this well!\n",
    "\n",
    "[Read this to understand broadcasting rules](https://numpy.org/doc/stable/user/basics.broadcasting.html).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e154eb-0c74-4412-a664-68c8823a63e3",
   "metadata": {},
   "source": [
    "# Neural networks usually start out with random values that need trained\n",
    "\n",
    "A model is **trained** by having its weights adjusted by small amounts so that the **accuracy** improves and the **loss** shrinks.  During training, it is common for the **loss** to grow instead of shrink in some cases.  This happens when the model potentially skips over the ideal weight combination or finds itself in a local minima.  \n",
    "\n",
    "\n",
    "# Gradient Descent and Learning Rate\n",
    "\n",
    "The process of applying gradient descent to a neural network deposits gradients to each parameter of the network.  The gradient is a floating point number indicating the direction the parameter in the network needs to change to improve **accuracy** and reduce **loss**.  Once the gradients are distributed through the network, they are applied by multiplying the **negative** gradient by the weight or bias and a **learning rate**.  The negative is why the process is called **descent**.  When numbers are multiplied and one of them is negative, it makes the entire result negative.  The gradient is like a slope and if the negative wasn't used, the result would continue to grow away from the desired result.\n",
    "\n",
    "# Optimizer\n",
    "\n",
    "The gradients are deposited to each weight and depending upon the network setup a multiple gradients may be deposited to the same weight.  When this happens, the gradients are simply summed together.  For this reason, before depositing gradients it is important to set them all to zero.  In PyTorch this is done by using an **optimizer**. \n",
    "\n",
    "# Learning Rate\n",
    "\n",
    "The **learning rate** is typically a small number 0.01E-4 to 0.01E-5.  The primary reason for the learning rate being so small is to prevent the model from skipping past the best settings.  There is a lot that can be done with learning rates including making them dynamic, altering them based upon the given epoch.\n",
    "\n",
    "# Epoch\n",
    "\n",
    "An epoch is one complete pass through the entire training dataset. During an epoch, the model's parameters are updated iteratively using subsets of the training data, often referred to as batches. Multiple epochs are usually necessary to sufficiently train a model.  In the code below, 2000 epochs are used training on the single equation `30+70 = 100`.\n",
    "\n",
    "# Hyperparameters\n",
    "\n",
    "A hyperparameter is a parameter whose value is set before the training process begins, as opposed to the parameters of the model, which are learned during training. Examples include learning rate, batch size, and number of epochs. Hyperparameters are often tuned to optimize model performance.\n",
    "\n",
    "# Loss function\n",
    "\n",
    "The loss function (often called criterion) is used to compare the model's prediction to the actual value.  In the example below, the loss function is the MSELoss() function.  The MSELoss function calculates the average of the squares of the differences between predicted and actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c80027-bea5-4d6f-8629-4d58cb026dfb",
   "metadata": {},
   "source": [
    "# Overfitting\n",
    "\n",
    "Overfitting occurs when a model learns the training data too well, capturing noise and anomalies, rather than the underlying pattern. As a result, it performs poorly on new, unseen data. Overfitting is often a sign that a model is too complex relative to the simplicity of the problem. Techniques like **regularization**, **dropout**, and **simpler architectures** can help mitigate overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbfb084-15ee-43b1-9cea-3d08b2f3d669",
   "metadata": {},
   "source": [
    "## Gradient Descent (more details)\n",
    "\n",
    "Models are trained using **gradient descent**.  The \"gradient\" in the name refers to the derivative of the function at the current point. The algorithm takes steps proportional to the negative of the gradient, moving towards the minimum of the function.\n",
    "\n",
    "The derivative of a function at a given point is essentially the slope of the function at that point. In the context of a function $ ( f(x) ) $ of a single variable, the derivative $ ( f'(x) ) $represents the slope of the tangent line to the function at a specific point $ ( x ) $. This slope indicates how the function is changing at that point. For functions of more than one variable, the concept of a derivative generalizes to partial derivatives.\n",
    "\n",
    "A partial derivative is the derivative of a function of multiple variables with respect to one of those variables, keeping the other variables constant. For a function $ ( f(x, y, z, \\ldots) ) $, the partial derivative with respect to $ ( x ) $ would indicate how $ ( f ) $ changes as $ ( x ) $ changes, while keeping $ ( y, z, \\ldots ) $ constant. It is denoted as $ ( \\frac{\\partial f}{\\partial x} ) $ for the variable $ ( x ) $.\n",
    "\n",
    "Partial derivatives are used to form the gradient vector, which combines all the partial derivatives of a function into a single vector. This is useful in multivariable calculus and optimization problems, including machine learning algorithms like gradient descent.\n",
    "\n",
    "The chain rule extends to functions of multiple variables through the use of partial derivatives. This generalizes to more complicated functions and is a cornerstone of backpropagation in neural networks, where it's used to compute gradients of a loss function with respect to the weights.\n",
    "\n",
    "[Andrej Karpathy's Zero to Hero course](https://karpathy.ai/zero-to-hero.html) explains this well!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a20bc1-4fea-4b29-8db4-a7fb64db9274",
   "metadata": {},
   "source": [
    "# Loss Functions\n",
    "\n",
    "Here's the list of common loss functions with their corresponding PyTorch functions:\n",
    "\n",
    "Each loss function has its own characteristics and is suited for specific types of problems. Choosing the right loss function is crucial for training an effective model.\n",
    "\n",
    "1. **Mean Squared Error (MSE)**\n",
    "   - **PyTorch Function**: `torch.nn.MSELoss()`\n",
    "   - **Description**: Calculates the average of the squares of the differences between predicted and actual values.\n",
    "   - **Best Suited For**: Regression problems.\n",
    "\n",
    "2. **Mean Absolute Error (MAE)**\n",
    "   - **PyTorch Function**: `torch.nn.L1Loss()`\n",
    "   - **Description**: Calculates the average of the absolute differences between predicted and actual values.\n",
    "   - **Best Suited For**: Regression problems with outliers.\n",
    "\n",
    "3. **Cross-Entropy Loss (Log Loss)**\n",
    "   - **PyTorch Function**: `torch.nn.CrossEntropyLoss()`\n",
    "   - **Description**: Measures the performance of a classification model, rewarding confidence in correct classifications.\n",
    "   - **Best Suited For**: Binary and multi-class classification.\n",
    "\n",
    "4. **Hinge Loss**\n",
    "   - **PyTorch Function**: `torch.nn.HingeEmbeddingLoss()`\n",
    "   - **Description**: Used for \"maximum-margin\" classification, particularly for support vector machines.\n",
    "   - **Best Suited For**: Binary classification.\n",
    "\n",
    "5. **Categorical Cross-Entropy**\n",
    "   - **PyTorch Function**: `torch.nn.CrossEntropyLoss()`\n",
    "   - **Description**: Extension of Cross-Entropy loss for multi-class classification problems.\n",
    "   - **Best Suited For**: Multi-class classification with single label.\n",
    "\n",
    "6. **Kullback-Leibler Divergence**\n",
    "   - **PyTorch Function**: `torch.nn.KLDivLoss()`\n",
    "   - **Description**: Measures how one probability distribution diverges from another.\n",
    "   - **Best Suited For**: Multi-class classification, recommendation systems.\n",
    "\n",
    "7. **Poisson Loss**\n",
    "   - **PyTorch Function**: `torch.nn.PoissonNLLLoss()`\n",
    "   - **Description**: Measures the difference between the predicted average occurrence rate and the actual rate.\n",
    "   - **Best Suited For**: Count-based regression problems.\n",
    "\n",
    "8. **Cosine Similarity**\n",
    "   - **PyTorch Function**: Use `torch.nn.CosineSimilarity()` and create a custom loss\n",
    "   - **Description**: Measures the cosine of the angle between the predicted and actual vectors to measure similarity.\n",
    "   - **Best Suited For**: Text similarity, clustering.\n",
    "\n",
    "9. **Huber Loss**\n",
    "   - **PyTorch Function**: `torch.nn.SmoothL1Loss()`\n",
    "   - **Description**: Combination of MAE and MSE; less sensitive to outliers than MSE.\n",
    "   - **Best Suited For**: Regression problems with occasional outliers.\n",
    "\n",
    "10. **Negative Log-Likelihood (NLL)**\n",
    "    - **PyTorch Function**: `torch.nn.NLLLoss()`\n",
    "    - **Description**: Similar to Cross-Entropy but without logit transformation; often used with Softmax.\n",
    "    - **Best Suited For**: Classification problems with Softmax.\n",
    "\n",
    "11. **Focal Loss**\n",
    "    - **PyTorch Function**: Not built-in; custom implementation required.\n",
    "    - **Description**: Modification of Cross-Entropy that gives more weight to hard-to-classify examples.\n",
    "    - **Best Suited For**: Imbalanced classification problems.\n",
    "\n",
    "12. **Dice Loss**\n",
    "    - **PyTorch Function**: Not built-in; custom implementation required.\n",
    "    - **Description**: Measures overlap between predicted and ground truth sets; often used in image segmentation.\n",
    "    - **Best Suited For**: Image segmentation tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17449993-34c6-4cc2-bfa8-0d41b1808225",
   "metadata": {},
   "source": [
    "# Optimizers\n",
    "\n",
    "Below are some common optimizers, their characteristics, and example PyTorch code snippets to initialize them:\n",
    "\n",
    "### SGD (Stochastic Gradient Descent)\n",
    "- **Why**: Simplicity and ease of implementation.\n",
    "- **Essence**: Updates parameters using the gradient of the loss function.\n",
    "- **Code**: \n",
    "  ```python\n",
    "  optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "  ```\n",
    "\n",
    "### Momentum\n",
    "- **Why**: Faster convergence compared to plain SGD.\n",
    "- **Essence**: Adds a momentum term to SGD, which considers past gradients.\n",
    "- **Code**:\n",
    "  ```python\n",
    "  optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "  ```\n",
    "\n",
    "### Adagrad\n",
    "- **Why**: Suitable for sparse data, adjusts learning rates.\n",
    "- **Essence**: Scales learning rate for each parameter individually.\n",
    "- **Code**:\n",
    "  ```python\n",
    "  optimizer = torch.optim.Adagrad(model.parameters(), lr=0.01)\n",
    "  ```\n",
    "\n",
    "### RMSprop\n",
    "- **Why**: Good for non-stationary objectives.\n",
    "- **Essence**: Adapts learning rates during training.\n",
    "- **Code**:\n",
    "  ```python\n",
    "  optimizer = torch.optim.RMSprop(model.parameters(), lr=0.01)\n",
    "  ```\n",
    "\n",
    "### Adam\n",
    "- **Why**: Good default for many problems.\n",
    "- **Essence**: Combines features of Momentum and RMSprop.\n",
    "- **Code**:\n",
    "  ```python\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "  ```\n",
    "\n",
    "### AdamW\n",
    "- **Why**: Improves generalization.\n",
    "- **Essence**: Similar to Adam but with weight decay.\n",
    "- **Code**:\n",
    "  ```python\n",
    "  optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "  ```\n",
    "\n",
    "Each of these optimizers can be more effective depending on the specific problem, architecture, or data you are working with."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
