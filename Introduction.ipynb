{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d250b8aa-80d2-4932-87a8-c438ec6554db",
   "metadata": {},
   "source": [
    "# Preamble\n",
    "\n",
    "Many of the ideas, text, and code are mine.  However, I have used ChatGPT to also generate ideas, code, and text.  ChatGPT often provides insights that I didn't think of or different ways of explaining things.  Overall, my goal is not to write a tutorial using only my own ideas, thoughts, and code, but rather to write something that is useful for understanding how deep learning at an intuitive level.  There will be terms which are thrown out which probably won't make much sense in the beginning.  I hope to explain them in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ce3242-3f0b-498e-9767-e54cc6eea31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchviz matplotlib graphviz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6049ddaa-a3d6-45ce-a61d-a160de52a3b0",
   "metadata": {},
   "source": [
    "# A Brief Overview of Neural Networks and AI\n",
    "\n",
    "Neural networks (NN) are computational **models** inspired by the human brain, designed to recognize patterns. They consist of interconnected **layers** of nodes, or \"**neurons**,\" that process and transmit information. Typically organized into **input**, **hidden**, and **output layers**, neural networks are **trained** on data to perform **tasks** like **classification**, **regression**, and **pattern recognition**. The training process involves adjusting the **weights** of connections based on the difference between the **predicted** and **actual output**, often using algorithms like **backpropagation**.\n",
    "\n",
    "A Deep Neural Network (DNN) is a type of neural network with multiple layers between the input and output layers. These additional layers enable the network to learn more complex features and representations, making DNNs well-suited for tasks like image and speech recognition, natural language processing, and more. The \"deep\" in DNN refers to the depth, or number of layers, in the network.\n",
    "\n",
    "Neural networks with at least one hidden layer are theoretically capable of approximating any **continuous function** to a high degree of **accuracy**, given enough neurons and appropriate training. This is known as the Universal Approximation Theorem. However, the practical ability to approximate any function depends on factors like **architecture**, **training data**, and **optimization algorithms**.\n",
    "\n",
    "Outside of neural networks, various other types of machine learning models exist, including:\n",
    "\n",
    "* Decision Trees: Used for classification and regression tasks.\n",
    "* Random Forest: **Ensemble** of decision trees that improves prediction accuracy.\n",
    "* Support Vector Machines (SVM): Effective for classification and some regression tasks.\n",
    "* Linear Regression: Used for predicting numerical values based on **linear relationships**.\n",
    "* Logistic Regression: Used for **binary classification** tasks.\n",
    "* K-Nearest Neighbors (K-NN): Classification or regression based on the closest data points.\n",
    "* Naive Bayes: Probabilistic classifier based on Bayes' theorem.\n",
    "* Gradient Boosting Machines: Ensemble method that combines multiple **weak learners**.\n",
    "* Hidden Markov Models: Used in **time series analysis** and **natural language processing (NLP)**.\n",
    "\n",
    "Each has its own strengths, weaknesses, and suitability for specific types of data and tasks.\n",
    "\n",
    "All of these machine learning models can act as functions for **prediction**. They take an input (or set of inputs) and produce an output based on the learned patterns in the data. The type of prediction—whether it's classification, regression, or clustering—depends on the specific model and how it's trained.\n",
    "\n",
    "They all take the form of f(x) => y where x is some input data.\n",
    "\n",
    "All of the above models or functions are **supervised**.  All this means is that to make the model work properly, they must be trained using input/expected output pairs.  For example, to train a model to add, it would need a bunch of input equations such as 1+1 and the corresponding expected output of 2.\n",
    "\n",
    "Some machine learning approaches are **unsupervised**.  These are generally **clustering** algorithms or algorithms which alter the **dimensionality of data**.\n",
    "\n",
    "* Hidden Markov Models: Used in time series analysis and natural language processing.\n",
    "* Clustering Algorithms (e.g., K-Means, Hierarchical): Used for unsupervised learning to group data.\n",
    "* Principal Component Analysis (PCA): Dimensionality reduction technique.\n",
    "* Gaussian Mixture Models: Probabilistic model for clustering.\n",
    "\n",
    "Note that Hidden Markov Models can be both supervised or unsupervised.\n",
    "\n",
    "This document will largely focus on neural networks (NN).  I find myself frequently using neural networks and deep neural networks interchangeably.  In general, most useful neural networks that are used today are in fact DNNs.  DNNs and NNs operate in the same manner.  There are just more operations in a DNN as it has more layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62885ed8-0b47-4cee-b947-d299463eda6d",
   "metadata": {},
   "source": [
    "# A Simple Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d1da51-323d-415f-949d-7e054c50c7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "print('3 + 5 =', add(3, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6082b53c-22c3-4bbd-8eff-6cf4a356e2bd",
   "metadata": {},
   "source": [
    "# Common Python Imports\n",
    "\n",
    "The following imports are used very frequent when working with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc5f97e-8193-477b-aecd-9b34f1f677a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "def set_random_seed(SEED):\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    np.random.seed(SEED)\n",
    "    random.seed(SEED)\n",
    "\n",
    "set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc8b5d5-bf1d-4f6c-85e2-6b91c5e51469",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "All of the **supervised** **models** above take **input**.  Input is like the a and b to the add function above.  They also produce an output.  In the function above, if 3 and 5 are passed as input, the function will return 8 as output.  A model's output is often called **output**, or a **prediction**. \n",
    "\n",
    "The function `add(a, b)` is deterministic and straightforward; it simply returns the sum of `a` and `b`. For this specific task of addition, it would always be correct, assuming that `a` and `b` are numbers or other types that support the addition operation. It's a simple, well-defined function with predictable behavior.\n",
    "\n",
    "Imagine if the add function above didn't include the logic `return a + b` and instead we asked the computer to figure out how to convert a and b to the proper output.\n",
    "\n",
    "If the add function lacked the logic `return a + b`, and you wanted the computer to \"learn\" how to perform addition, you'd essentially be asking for a machine learning model to approximate the addition function. Through `supervised learning`, you could train the model on numerous examples of addition problems (input pairs a, b) and their corresponding sums (outputs). Once trained, the model could then \"infer\" or predict the sum for any new pair of numbers a and b, effectively approximating the addition function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b173ddee-2cc3-4915-b69d-fce3bc782d7a",
   "metadata": {},
   "source": [
    "# Models look like functions but they are not quite the same\n",
    "\n",
    "**A model is a collection of layers and operations organized in a specific architecture, often used to map inputs to outputs**. It may include **learnable parameters**, and is usually designed for **tasks like classification, regression, etc**.\n",
    "\n",
    "A function is a block of reusable code that performs a specific operation. It doesn't have learnable parameters and does the same operation each time it's called.\n",
    "\n",
    "In machine learning, a model can be considered a complex function with learnable parameters.\n",
    "\n",
    "A model is **trained** to perform a **task**.  Once the model is sufficiently **accurate**, it can then be used like a function to produce results.  A function typically will always produce the right and consistent answer if it is programmed to do so through a series of well designed steps.  A model on the other hand relies on being trained through lots of data and less on those steps.  For example, the same model might be trained to do different things.\n",
    "\n",
    "It is often the case that a model might be 99% accurate.  All this means is that in 99 out of a 100 cases, it will produce the right answer AND in 1 out of every 100 cases, it will produce the wrong answer.  Models usually require more computational resources to arrive at the answer than functions.  Functions in general should be used when there is a way to programatically compute the answer in a reliable manner.  Models should be used when writing such functions would be arbitrarily complex.\n",
    "\n",
    "It doesn't make sense to write a neural network to solve an addition problem.  However, it can help to illustrate the differences and important concepts related to machine learning.\n",
    "\n",
    "The first concept introduced here is a model which supports adding two numbers together and finally outputting the sum of those two numbers.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f1ed80-0ace-45f4-8435-96ab33a02a48",
   "metadata": {},
   "source": [
    "# The First Model\n",
    "\n",
    "The model below likely won't make a lot of sense at first.  It consists of layers of neurons (which are just floating point numbers) which will be organized to provide the final answer.\n",
    "\n",
    "The input layer maps the two inputs (a + b) into the `network` of numbers.  This input mapping layer is the first layer.  The next layer consists of 4 weights (the output parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bc4b8f-ecec-4010-aa10-b0e0044eb93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model\n",
    "class Adder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Adder, self).__init__()\n",
    "        self.input = torch.nn.Linear(2, 4)\n",
    "        self.hidden = torch.nn.Linear(4, 4)\n",
    "        self.output = torch.nn.Linear(4, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input(x)\n",
    "        x = self.hidden(x)\n",
    "        return self.output(x)\n",
    "\n",
    "\n",
    "# Calling this to make the work repeatable\n",
    "set_random_seed(42)\n",
    "model = Adder()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c3d311-ca4d-4484-9b1a-c31bdc122418",
   "metadata": {},
   "source": [
    "# Visualizing Neural Networks\n",
    "\n",
    "The following code is very useful for visualizing deep neural networks.  At the end, the draw_dnn function is called to visualize the Adder model above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0b9771-f63e-40f0-b1a3-e29502b178d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_dnn(layers):\n",
    "    fig, ax = plt.subplots()\n",
    "    for i, n_neurons in enumerate(layers):\n",
    "        y_offset = -n_neurons / 2.0 + 0.5\n",
    "        for j in range(n_neurons):\n",
    "            plt.scatter(i, j + y_offset, s=500, zorder=2)\n",
    "            if i == 0:\n",
    "                plt.text(i, j + y_offset, 'Input', verticalalignment='center', horizontalalignment='right')\n",
    "            elif i == len(layers) - 1:\n",
    "                plt.text(i, j + y_offset, 'Output', verticalalignment='center', horizontalalignment='right')\n",
    "            else:\n",
    "                plt.text(i, j + y_offset, f'Hidden{i}', verticalalignment='center', horizontalalignment='right')\n",
    "    \n",
    "    for i in range(len(layers) - 1):\n",
    "        y_offset_src = -layers[i] / 2.0 + 0.5\n",
    "        y_offset_dst = -layers[i+1] / 2.0 + 0.5\n",
    "        for j in range(layers[i]):\n",
    "            for k in range(layers[i+1]):\n",
    "                plt.plot([i, i+1], [j + y_offset_src, k + y_offset_dst], c='black', zorder=1)\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "draw_dnn([2, 4, 4, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05319aa3-9d46-4857-b686-e4132e76ac7c",
   "metadata": {},
   "source": [
    "# Testing the model\n",
    "\n",
    "We can now test our new Adder model.  If all is well, then it should be able to answer with 100 (30 + 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ff6ce6-ece8-4776-a267-0df8b4505307",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward(torch.Tensor([[30, 70]])).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51aff84-4c69-4b71-9da7-139b4163c098",
   "metadata": {},
   "source": [
    "# The model isn't ready\n",
    "\n",
    "30 + 70 != 1.186\n",
    "\n",
    "The model is initially set to random numbers.  The following code shows the values of the layers of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386bcc12-44b9-43c1-ab80-ec17f273b7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print weights and biases\n",
    "for name, param in model.named_parameters():\n",
    "    print(f'{name}:\\n{param.data}\\n{param.grad}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e31a923-9c4d-4bb3-8ae1-a972b4583df1",
   "metadata": {},
   "source": [
    "The forward calculation works in the following way:\n",
    "\n",
    "1. **Input Layer**: Linear transformation $ Y = X \\cdot W_{\\text{input}} + b_{\\text{input}} $\n",
    "    - $( W_{\\text{input}}) $ and $ ( b_{\\text{input}}) $ are the weight and bias matrices for the input layer.\n",
    "    - Input \\( X = [30, 70] \\), \\( Y \\) would be a 1x4 tensor.\n",
    "    - $ Y = [30, 70] \\cdot W_{\\text{input}} + b_{\\text{input}} $\n",
    "<br/>\n",
    "2. **Hidden Layer**: Linear transformation $ Y_{\\text{hidden}} = Y \\cdot W_{\\text{hidden}} + b_{\\text{hidden}} $\n",
    "    - $( W_{\\text{hidden}})$ and $( b_{\\text{hidden}} )$ are the weight and bias matrices for the hidden layer.\n",
    "    - $ ( Y_{\\text{hidden}} ) $ would be a 1x4 tensor.\n",
    "    - $ Y_{\\text{hidden}} = Y \\cdot W_{\\text{hidden}} + b_{\\text{hidden}} $\n",
    "<br/>\n",
    "3. **Output Layer**: Linear transformation $ Y_{\\text{output}} = Y_{\\text{hidden}} \\cdot W_{\\text{output}} + b_{\\text{output}} $\n",
    "    - $ ( W_{\\text{output}} ) $ and $ ( b_{\\text{output}} ) $ are the weight and bias matrices for the output layer.\n",
    "    - $ ( Y_{\\text{output}} ) $ would be a 1x1 tensor.\n",
    "    - $ Y_{\\text{output}} = Y_{\\text{hidden}} \\cdot W_{\\text{output}} + b_{\\text{output}} $\n",
    "\n",
    "All the weight and bias matrices are initialized as random numbers.\n",
    "\n",
    "The following code shows how to manually run the steps above and then compares it to calling the model directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52abc2fe-e3ec-42d0-9878-32fab632acd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input tensor\n",
    "input_tensor = torch.Tensor([[30, 70]])\n",
    "\n",
    "# Extract weights and biases\n",
    "W_input = model.input.weight\n",
    "b_input = model.input.bias\n",
    "\n",
    "W_hidden = model.hidden.weight\n",
    "b_hidden = model.hidden.bias\n",
    "\n",
    "W_output = model.output.weight\n",
    "b_output = model.output.bias\n",
    "\n",
    "print(f\"input_tensor.shape: {input_tensor.shape}\")\n",
    "print(f\"W_input.shape: {W_input.shape}\")\n",
    "\n",
    "# Step 1: Input Layer\n",
    "# multiply the input_tensor by the transposed W_input weights and then add the b_input bias\n",
    "Y_input = torch.matmul(input_tensor, W_input.t()) + b_input\n",
    "print(f\"After Input Layer: {Y_input}\")\n",
    "\n",
    "# Step 2: Hidden Layer\n",
    "Y_hidden = torch.matmul(Y_input, W_hidden.t()) + b_hidden\n",
    "print(f\"After Hidden Layer: {Y_hidden}\")\n",
    "\n",
    "# Step 3: Output Layer\n",
    "Y_output = torch.matmul(Y_hidden, W_output.t()) + b_output\n",
    "print(f\"After Output Layer: {Y_output}\")\n",
    "\n",
    "# Verify with the forward pass of the model\n",
    "model_output = model(input_tensor)\n",
    "print(f\"Model's Forward Pass Output: {model_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0873d37-00ba-4626-9743-a23511736b0f",
   "metadata": {},
   "source": [
    "The output from using the 3 steps is the same as calling the model directly.  `model(input_tensor)` is equivalent to `model.forward(input_tensor)`. \n",
    "\n",
    "# Neural networks consist of many Linear Transformations\n",
    "\n",
    "Notice that in each layer the computation is basically the same taking the form\n",
    "\n",
    "$ Y = X \\cdot W + b $\n",
    "\n",
    "**W** and **b** are the weight and bias matrices and **X** is the input matrix.  **Y** is the output of the **linear transformation**.\n",
    "\n",
    "A linear transformation is a way to change a set of points (vectors) in space so that lines remain lines, and the origin remains fixed. In simpler terms, it's a rule for moving every point to a specific new location without twisting, warping, or tearing the shape formed by those points. In machine learning, this is often done using matrices to systematically shift, rotate, or scale the input data.\n",
    "\n",
    "In the above code, the function used the `t()` function which transposes a matrix.  In the first instance, the input_tensor has a **shape** of (1,2) and the W_input has a shape of (4, 2).  Matrix multiplication requires that **broadcasting rules** be followed.  Taking the transposed form of W_input changes its shape to (2, 4).  The result (Y_input) of the matrix multiplication is a matrix of (1, 4).  \n",
    "\n",
    "```\n",
    "Y_input = torch.matmul(input_tensor, W_input.t()) + b_input\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53f6b11-521b-4867-a71e-c1f65a054555",
   "metadata": {},
   "source": [
    "# Broadcasting Rules\n",
    "\n",
    "If you understand broadcasting rules, it can help to make sense of when to transpose a matrix and what types of shape transformations can happen.  [Andrej Karpathy's Zero to Hero course](https://karpathy.ai/zero-to-hero.html) explains this well!\n",
    "\n",
    "[Read this to understand broadcasting rules](https://numpy.org/doc/stable/user/basics.broadcasting.html).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e154eb-0c74-4412-a664-68c8823a63e3",
   "metadata": {},
   "source": [
    "# Neural networks usually start out with random values that need trained\n",
    "\n",
    "A model is **trained** by having its weights adjusted by small amounts so that the **accuracy** improves and the **loss** shrinks.  During training, it is common for the **loss** to grow instead of shrink in some cases.  This happens when the model potentially skips over the ideal weight combination or finds itself in a local minima.  \n",
    "\n",
    "\n",
    "# Gradient Descent and Learning Rate\n",
    "\n",
    "The process of applying gradient descent to a neural network deposits gradients to each parameter of the network.  The gradient is a floating point number indicating the direction the parameter in the network needs to change to improve **accuracy** and reduce **loss**.  Once the gradients are distributed through the network, they are applied by multiplying the **negative** gradient by the weight or bias and a **learning rate**.  The negative is why the process is called **descent**.  When numbers are multiplied and one of them is negative, it makes the entire result negative.  The gradient is like a slope and if the negative wasn't used, the result would continue to grow away from the desired result.\n",
    "\n",
    "# Optimizer\n",
    "\n",
    "The gradients are deposited to each weight and depending upon the network setup a multiple gradients may be deposited to the same weight.  When this happens, the gradients are simply summed together.  For this reason, before depositing gradients it is important to set them all to zero.  In PyTorch this is done by using an **optimizer**. \n",
    "\n",
    "# Learning Rate\n",
    "\n",
    "The **learning rate** is typically a small number 0.01E-4 to 0.01E-5.  The primary reason for the learning rate being so small is to prevent the model from skipping past the best settings.  There is a lot that can be done with learning rates including making them dynamic, altering them based upon the given epoch.\n",
    "\n",
    "# Epoch\n",
    "\n",
    "An epoch is one complete pass through the entire training dataset. During an epoch, the model's parameters are updated iteratively using subsets of the training data, often referred to as batches. Multiple epochs are usually necessary to sufficiently train a model.  In the code below, 2000 epochs are used training on the single equation `30+70 = 100`.\n",
    "\n",
    "# Hyperparameters\n",
    "\n",
    "A hyperparameter is a parameter whose value is set before the training process begins, as opposed to the parameters of the model, which are learned during training. Examples include learning rate, batch size, and number of epochs. Hyperparameters are often tuned to optimize model performance.\n",
    "\n",
    "# Loss function\n",
    "\n",
    "The loss function (often called criterion) is used to compare the model's prediction to the actual value.  In the example below, the loss function is the MSELoss() function.  The MSELoss function calculates the average of the squares of the differences between predicted and actual values.\n",
    "\n",
    "At the end of the code below, 99.9778 is predicted for 30+70.  The model seems to be performing well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a136337-88fe-4211-81f8-d9accf912bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seed(42)\n",
    "model = Adder()\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.0002\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "epochs = 2000\n",
    "\n",
    "# Create one row with [30, 70]\n",
    "input_data = torch.Tensor([[30, 70]])\n",
    "\n",
    "# Compute the sum of each row in input_data\n",
    "target = torch.sum(input_data, dim=1, keepdim=True)\n",
    "\n",
    "for i in range(epochs):\n",
    "    # Get a prediction from the model\n",
    "    prediction = model(input_data)\n",
    "\n",
    "    # Compute loss by comparing the prediction to target\n",
    "    loss = criterion(prediction, target)\n",
    "\n",
    "    # Print the first epoch, and one for every 10% thereafter.\n",
    "    print(f\"[{i+1}] Prediction: {prediction.item()}, Loss: {loss.item()}\") if i == 0 or i % (epochs / 10) == (epochs / 10)-1 else None\n",
    "        \n",
    "    # Zero the gradients so that they can accumulate during loss.backward()\n",
    "    optimizer.zero_grad()\n",
    "    # Set all of the gradients in the network based upon the loss\n",
    "    loss.backward()\n",
    "    # Alter the weights in the network based upon the accumulated gradients and the learning rate using the optimizer\n",
    "    optimizer.step()\n",
    "\n",
    "model(torch.Tensor([[30, 70]])).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c80027-bea5-4d6f-8629-4d58cb026dfb",
   "metadata": {},
   "source": [
    "# Overfitting\n",
    "\n",
    "Overfitting occurs when a model learns the training data too well, capturing noise and anomalies, rather than the underlying pattern. As a result, it performs poorly on new, unseen data. Overfitting is often a sign that a model is too complex relative to the simplicity of the problem. Techniques like **regularization**, **dropout**, and **simpler architectures** can help mitigate overfitting.\n",
    "\n",
    "Notice how that the model has learned to answer 30+70 pretty well, but the other equations aren't right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f15f11-687c-4ac0-9e0a-3e866bff0fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, a, b):\n",
    "    return model(torch.Tensor([[a, b]])).item()\n",
    "\n",
    "print(f\"30 + 70 = {test_model(model, 30, 70):0.2f}\")\n",
    "print(f\"70 + 30 = {test_model(model, 70, 30):0.2f}\")\n",
    "print(f\"130 + -30 = {test_model(model, 130, -30):0.2f}\")\n",
    "print(f\"3 + 7 = {test_model(model, 3, 7):0.2f}\")\n",
    "print(f\"33 + 7 = {test_model(model, 33, 7):0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01315231-92dc-41e6-9eae-15da7cd6e48b",
   "metadata": {},
   "source": [
    "# Providing lots of examples for training\n",
    "\n",
    "In this case, the model learned from a single example.  It learned to produce 100 when 30 and 70 were presented to it, but as is evident from above, it didn't learn addition.  Notice that 30+70 != 70+30!\n",
    "\n",
    "We can use torch to generate a bunch of examples to train on along with their targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb7e044-56a8-479e-8991-93b2ace1e62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 10,000 random pairs between -500 and 500\n",
    "input_data = (torch.rand(10000, 2) * 1000) - 500\n",
    "\n",
    "# Calculate the target sum for each pair\n",
    "target = torch.sum(input_data, dim=1, keepdim=True)\n",
    "\n",
    "print(input_data[0], target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6cb9ab-3baf-4848-8384-287bd6ab47a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seed(42)\n",
    "model = Adder()\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.0002\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "epochs = 2000\n",
    "\n",
    "# Generate 10,000 random pairs between -500 and 500\n",
    "input_data = (torch.rand(10000, 2) * 1000) - 500\n",
    "\n",
    "# Calculate the target sum for each pair\n",
    "target = torch.sum(input_data, dim=1, keepdim=True)\n",
    "\n",
    "for i in range(epochs):\n",
    "    # Get a prediction from the model\n",
    "    prediction = model(input_data)\n",
    "\n",
    "    # Compute loss by comparing the prediction to target\n",
    "    loss = criterion(prediction, target)\n",
    "\n",
    "    # Print the first epoch, and one for every 10% thereafter.\n",
    "    print(f\"[{i+1}] Loss: {loss.item()}\") if i == 0 or i % (epochs / 10) == (epochs / 10)-1 else None\n",
    "        \n",
    "    # Zero the gradients so that they can accumulate during loss.backward()\n",
    "    optimizer.zero_grad()\n",
    "    # Set all of the gradients in the network based upon the loss\n",
    "    loss.backward()\n",
    "    # Alter the weights in the network based upon the accumulated gradients and the learning rate using the optimizer\n",
    "    optimizer.step()\n",
    "\n",
    "print(f\"30 + 70 = {test_model(model, 30, 70):0.2f}\")\n",
    "print(f\"70 + 30 = {test_model(model, 70, 30):0.2f}\")\n",
    "print(f\"130 + -30 = {test_model(model, 130, -30):0.2f}\")\n",
    "print(f\"3 + 7 = {test_model(model, 3, 7):0.2f}\")\n",
    "print(f\"33 + 7 = {test_model(model, 33, 7):0.2f}\")\n",
    "print(f\"-330000 + 30000 = {test_model(model, -330000, 30000):0.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
